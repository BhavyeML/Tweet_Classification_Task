{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport pandas as pd\nimport csv\nimport numpy as np\nimport nltk\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nimport nltk.corpus \nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, f1_score, classification_report\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.model_selection import learning_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom pprint import pprint\nimport string \nimport re \nimport gc\nimport glob\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":6,"outputs":[{"output_type":"stream","text":"/kaggle/input/zip-tweets/Data/cricket_hashtag.csv\n/kaggle/input/zip-tweets/Data/football_hashtag.csv\n/kaggle/input/zip-tweets/Data/machine learning_hashtag.csv\n/kaggle/input/zip-tweets/Data/mobiles_hashtag.csv\n/kaggle/input/zip-tweets/Data/happy birthday_hashtag.csv\n/kaggle/input/zip-tweets/Data/bollywood_hashtag.csv\n/kaggle/input/zip-tweets/Data/Politics_hashtag.csv\n/kaggle/input/zip-tweets/Data/hollywood_hashtag.csv\n/kaggle/input/zip-tweets/Data/bigboss_hashtag.csv\n/kaggle/input/zip-tweets/Data/food_hashtag.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Stitching all files together"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndef stitch_csv(dirname):\n    extension='csv'\n    all_files=[i for i in glob.glob(os.path.join(dirname,'*.{}'.format(extension)))]\n    combined_file=[]\n    for f in all_files:\n        file=pd.read_csv(f)\n        file['label']=os.path.splitext(os.path.basename(f))[0]\n        combined_file.append(file)\n    combined_csv=pd.concat(combined_file,ignore_index=True)\n    return combined_csv\n\ncsv_file=stitch_csv(dirname)","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Check"},{"metadata":{"trusted":true},"cell_type":"code","source":"#data observation\n\ndef data_obs(data):\n    print(\"dataset size:\")\n    print(data.shape)\n    print(data.head(10))\n\n\ndata_obs(csv_file) #function can be used to check the dimensions of dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#distribution of classes for prediction\ndef create_distribution(dataFile):\n    \n    return sb.countplot(x='label', data=dataFile, palette='hls')\n\ncreate_distribution(csv_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data integrity check i.e, we need to check for null entries in all the three variables.\n#none of the datasets contains missing values therefore no cleaning required\ndef data_qualityCheck(data):\n    \n    print(\"Check Started...\")\n    print(data.isnull().sum())\n    print(\"Check Started...\")\n    data.info()\n        \n    print(\"Check finished.\")\n\n    \n# data_qualityCheck() can be run to see the quality check results as well as integrity constraints.\ndata_qualityCheck(csv_file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML Pre-Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"text=pd.Series(csv_file.iloc[:]['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(csv_file.iloc[:10]['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting label values into no. for classification purpose\ndef to_label(data):\n    lst=[]\n    for label in data:\n        \n        if label == \"happy birthday_hashtag\":\n            lst.append(0)\n        elif label == \"food_hashtag\":\n            lst.append(1)\n        elif label == \"football_hashtag\":\n            lst.append(2)\n        elif label == \"cricket_hashtag\":\n            lst.append(3)\n        elif label == \"Politics_hashtag\":\n            lst.append(4)\n        elif label == \"machine learning_hashtag\":\n            lst.append(5)\n        elif label == \"hollywood_hashtag\":\n            lst.append(6)\n        elif label == \"bollywood_hashtag\":\n            lst.append(7)\n        elif label == \"bigboss_hashtag\":\n            lst.append(8)\n        elif label == \"mobiles_hashtag\":\n            lst.append(9)\n        else:\n            print(\"AAAAAA\")\n    return lst\n    \n\nlabel=pd.Series(to_label(csv_file['label']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n        \n        if text:\n            \n            text1 = re.sub('^b','', str(text))\n            text1 = re.sub(r'(\\\\x[a-z]*[0-9]*)','', str(text1))\n            text2=re.sub('(\\s[hhtps:]\\S)\\s','', str(text1))\n            text2=text2.lower()\n        else:\n            pass\n\n        return text2\n\nfor i,j in enumerate(text):\n    text[i]=clean_text(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#performing train test split\ntrain_x,test_x,train_y,test_y=train_test_split(text,label,test_size=0.1,random_state=1)\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML Feature-Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nBefore we can train an algorithm to classify labels, we need to extract features from it. It means reducing the mass\nof unstructured data into some uniform set of attributes that an algorithm can understand. For tweet classification, it could be \ntf-idf along with n-grams. \n\"\"\"\n\n\n\n#we will start with simple bag of words technique \n#creating feature vector - document term matrix\ncountV = CountVectorizer()\ntrain_count = countV.fit_transform(train_x.values)\n\n\n\n#print training doc term matrix\ndef get_countVectorizer_stats():\n    \n    #vocab size\n    print(train_count.shape)\n\n    #check vocabulary using below command\n    #print(countV.vocabulary_)\n\n    #get feature names\n    #print(countV.get_feature_names()[:25])\nget_countVectorizer_stats()\n\n#create tf-df frequency features\n#tf-idf \ntfidfV = TfidfTransformer()\ntrain_tfidf = tfidfV.fit_transform(train_count)\n\ndef get_tfidf_stats():\n    print(train_tfidf.shape)\n    #get train data feature names \n    print(train_tfidf.A[:10])\n    \nget_tfidf_stats()\n\n\ntfidf_ngram = TfidfVectorizer(stop_words='english',ngram_range=(1,4),use_idf=True,smooth_idf=True)\n\n     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML Model And Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#naive-bayes classifier\nnb_pipeline_ngram = Pipeline([\n        ('nb_tfidf',tfidf_ngram),\n        ('nb_clf',MultinomialNB())])\n\nnb_pipeline_ngram.fit(train_x,train_y)\npredicted_nb_ngram = nb_pipeline_ngram.predict(test_x)\nprint(np.mean(predicted_nb_ngram == test_y))\n\n#========================================================================================\n\n#=========================================================================================\n\nprint(classification_report(test_y, predicted_nb_ngram))\n\nprint(test_y.shape)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk import word_tokenize\n\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n    \n#    text = re.sub(r'\\W+', '', text)\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n    return text\ncsv_file['text'] = csv_file['text'].apply(clean_text)\ncsv_file['text'] = csv_file['text'].str.replace('\\d+', '')\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 250\n# This is fixed.\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(csv_file['text'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":7,"outputs":[{"output_type":"stream","text":"Found 64002 unique tokens.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tokenizer.texts_to_sequences(csv_file['text'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","execution_count":8,"outputs":[{"output_type":"stream","text":"Shape of data tensor: (30000, 250)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(csv_file['label']).values\nprint('Shape of label tensor:', Y.shape)","execution_count":9,"outputs":[{"output_type":"stream","text":"Shape of label tensor: (30000, 10)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":12,"outputs":[{"output_type":"stream","text":"(27000, 250) (27000, 10)\n(3000, 250) (3000, 10)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":15,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 250, 100)          5000000   \n_________________________________________________________________\nspatial_dropout1d_1 (Spatial (None, 250, 100)          0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 100)               80400     \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 5,081,410\nTrainable params: 5,081,410\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 5\nbatch_size =32\n\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","execution_count":17,"outputs":[{"output_type":"stream","text":"Epoch 1/5\n760/760 [==============================] - 290s 381ms/step - loss: 0.1824 - accuracy: 0.9464 - val_loss: 0.1763 - val_accuracy: 0.9496\nEpoch 2/5\n760/760 [==============================] - 276s 363ms/step - loss: 0.0527 - accuracy: 0.9847 - val_loss: 0.1781 - val_accuracy: 0.9481\nEpoch 3/5\n760/760 [==============================] - 285s 375ms/step - loss: 0.0228 - accuracy: 0.9926 - val_loss: 0.1943 - val_accuracy: 0.9481\nEpoch 4/5\n760/760 [==============================] - 278s 366ms/step - loss: 0.0171 - accuracy: 0.9931 - val_loss: 0.2160 - val_accuracy: 0.9478\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Prediction Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"lst=[]\nfor i in predicted_val:\n    lst.append(np.argmax(i))\nPred_Y=pd.get_dummies(lst).values\npredicted_val = model.predict(X_test)\nprint(np.mean(Pred_Y ==Y_test))\n\n#========================================================================================\n\n#=========================================================================================\n\nprint(classification_report(Y_test, Pred_Y))\n\nprint(Y_test.shape)\n\n\n","execution_count":28,"outputs":[{"output_type":"stream","text":"0.9918\n              precision    recall  f1-score   support\n\n           0       0.96      0.82      0.88        83\n           1       0.99      0.99      0.99       442\n           2       0.96      0.96      0.96       517\n           3       0.99      0.96      0.98       485\n           4       0.91      0.97      0.94       268\n           5       0.96      0.97      0.96       510\n           6       0.94      0.93      0.94       151\n           7       0.94      0.95      0.95       483\n           8       1.00      0.78      0.88        27\n           9       0.97      0.91      0.94        34\n\n   micro avg       0.96      0.96      0.96      3000\n   macro avg       0.96      0.93      0.94      3000\nweighted avg       0.96      0.96      0.96      3000\n samples avg       0.96      0.96      0.96      3000\n\n(3000, 10)\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}